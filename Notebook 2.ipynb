# %% [markdown]
# ## 1. Importul bibliotecilor necesare
# 
# Importăm bibliotecile pandas, matplotlib.pyplot și seaborn pentru analiza și vizualizarea datelor.

# %%
# Importăm bibliotecile
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# %% [markdown]
# ## 2. Importul și explorarea datelor
# 
# În această secțiune am încărcat fișierul CSV cu datele și vom afișa primele 5 rânduri pentru a verifica structura datasetului.

# %%
# Citim fișierul CSV
df = pd.read_csv("Baza_date.csv")

# %%
# Afișăm primele 5 rânduri din baza de date pentru a verifica că s-a încărcat corect
print("Primele 5 rânduri din baza de date:")
print(df.head())

# %% [markdown]
# ## 3. Calcularea statisticilor descriptive
# 
# Calcularea și afișare statisticilor descriptive pentru variabilele cantitative (media, minim, maxim, etc.).

# %%
# Previzualizare date
print("----- Primele 5 rânduri -----")
print(df.head())

# Informații despre tipurile de date
print("\n----- Info despre dataframe -----")
print(df.info())

# Verificare valori lipsă
print("\n----- Valori lipsă pe coloană -----")
print(df.isnull().sum())

# Statistici descriptive (media, min, max, etc.)
print("\nStatistici descriptive:\n")
print(df.describe())

# %% [markdown]
# ## 3.1. Comentarea valorilor statisticilor descriptive în contextul analizei regresive
# 
# Valorile statisticilor descriptive oferă o imagine de ansamblu asupra distribuției indicatorilor socio-economici și demografici în cele 180 de țări analizate. Iată cum pot fi interpretate aceste valori în contextul analizei regresive:
# 
# - **Media** indică nivelul mediu al fiecărui indicator. Dacă media este semnificativ diferită de mediană, acest lucru poate sugera existența unor valori extreme (outliers) care influențează distribuția.
# - **Mediana** arată valoarea de mijloc și este mai puțin sensibilă la extreme. O diferență mare între medie și mediană poate indica o distribuție asimetrică.
# - **Modul** evidențiază valorile cele mai frecvente, util pentru a identifica dacă există grupuri de țări cu caracteristici similare.
# - **Deviația standard** măsoară dispersia valorilor față de medie. O deviație standard mare sugerează o variabilitate ridicată între țări, ceea ce poate afecta robustețea modelelor regresive.
# - **Minimul și maximul** arată intervalul de variație al fiecărui indicator. Prezența unor valori minime sau maxime extreme poate indica necesitatea de a trata outlierii înainte de analiza regresivă.
# 
# **Implicarea pentru analiza regresivă:**  
# - O dispersie mare a indicatorilor poate duce la relații regresive mai slabe sau la influența disproporționată a outlierilor.
# - Dacă indicatorii prezintă o distribuție asimetrică sau valori extreme, este recomandat să se aplice transformări (de exemplu, logaritmice) sau să se analizeze separat grupurile de țări cu caracteristici similare.
# - Relațiile potențiale între variabile pot fi investigate inițial prin corelații și vizualizări, folosind aceste statistici descriptive ca punct de plecare pentru selecția variabilelor relevante în modelele regresive.
# 
# Prin urmare, interpretarea atentă a acestor statistici este esențială pentru a construi modele regresive robuste și relevante pentru analiza dezvoltării umane în context global.

# %% [markdown]
# 
# - **Populație și Număr de Nașteri**: Ambele variabile prezintă o medie mult mai mare decât mediana, iar deviația standard este foarte ridicată. Acest lucru indică existența unor țări cu populații și număr de nașteri extrem de mari (outlieri, precum China sau India), care pot distorsiona relațiile regresive. 
# 
# - **Acces la Electricitate (%)**: Media este relativ ridicată, dar valoarea minimă este foarte mică, iar distribuția este asimetrică. Aceasta sugerează că există țări cu acces extrem de redus la electricitate, ceea ce poate influența semnificativ indicatorii de dezvoltare umană. În regresie, această variabilă poate avea o relație nelineară cu HDI sau speranța de viață.
# 
# - **Natalitate (pop/nr nașteri)**: Valorile minime și maxime sunt foarte îndepărtate de medie, indicând o dispersie mare între țări. Acest lucru reflectă diferențe majore în politicile demografice și nivelul de dezvoltare, ceea ce poate genera relații complexe în modelele regresive.
# 
# - **Speranța de viață la naștere**: Deviația standard este moderată, dar există outlieri spre valori mici, semnalând țări cu probleme grave de sănătate publică. Aceste extreme pot afecta coeficienții regresiei dacă nu sunt tratate corespunzător.
# 
# - **Ani așteptați de școlarizare**: Majoritatea țărilor au valori apropiate de medie, dar există și câteva outlieri cu valori foarte mici. Acest lucru poate evidenția inegalități educaționale care trebuie luate în considerare în analiza relației cu dezvoltarea umană.
# 
# 

# %% [markdown]
# ## 4. Analiza exploratorie a datelor

# %%
# Lista variabilelor cantitative 
variabile_cantitative = ['AccesElectricitate%', 'Populatie', 'NrNasteri', 
                         'Natalitate_(pop/nr,nasteri)', 'LifeExpectancyAtBirth', 'ExpectedYearsOfSchooling']

# Setăm dimensiunea figurii și layout-ul subploturilor
fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(15, 12))
axs = axs.ravel()  # transformăm matricea 2D de axe într-o listă simplă

# Iterăm prin fiecare variabilă și o desenăm
for i, var in enumerate(variabile_cantitative):
    axs[i].hist(df[var].dropna(), bins=15, color='skyblue', edgecolor='black')
    axs[i].set_title(f'Histograma pentru {var}', fontsize=12)
    axs[i].set_xlabel(var)
    axs[i].set_ylabel('Frecvență')

# Eliminăm subploturile goale (dacă sunt mai multe subploturi decât variabile)
for j in range(i+1, len(axs)):
    fig.delaxes(axs[j])

# Ajustăm spațierea
plt.tight_layout()

# Salvăm imaginea
plt.savefig("histograme_variabile_cantitative.png", dpi=300)

# Afișăm imaginea
plt.show()

# %% [markdown]
# Histogramele realizate pentru variabilele cantitative oferă o imagine de ansamblu asupra distribuției acestora în cadrul setului de date. 
# - AccesElectricitate%: Distribuția este puternic asimetrică spre stânga, majoritatea țărilor având un procent redus de acces la electricitate, ceea ce evidențiază inegalități majore în infrastructură.
# - Populatie și NrNasteri: Aceste variabile prezintă valori extreme (outlieri), cu câteva țări având populații și număr de nașteri mult mai mari decât restul, ceea ce poate influența media și alte statistici descriptive.
# - Natalitate_(pop/nr,nasteri): Distribuția natalității arată variații semnificative între țări, sugerând diferențe în politicile demografice și condițiile socio-economice.
# - LifeExpectancyAtBirth și ExpectedYearsOfSchooling: Majoritatea țărilor se concentrează în intervale relativ restrânse, dar există și aici variații notabile, reflectând diferențe în sistemele de sănătate și educație.
# 
# Aceste observații pot ghida analize suplimentare, precum identificarea factorilor care determină aceste variații sau corelarea lor cu alți indicatori socio-economici.

# %%
# Heatmap cu corelațiile dintre variabile numerice
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Matricea de corelație între variabile")
plt.show()

# %% [markdown]
# 
# 
#  **Corelație foarte puternică între Populație și Număr de Nașteri** (coeficient aproape de 1): Acest lucru era de așteptat, deoarece țările cu populație mare au, în general, și un număr mare de nașteri. Într-un model de regresie, includerea ambelor variabile poate duce la multicoliniaritate, deci este recomandat să alegi doar una ca predictor sau să folosești o variabilă derivată (ex: rata natalității).
# 
# - **Corelație pozitivă între AccesElectricitate% și LifeExpectancyAtBirth** (coeficient ridicat, ex: 0.7-0.8): Țările cu acces crescut la electricitate tind să aibă și o speranță de viață mai mare, ceea ce sugerează că infrastructura este un factor important pentru dezvoltarea umană. Această relație poate fi exploatată în modelul de regresie pentru a explica variația HDI sau a speranței de viață.
# 
# - **Corelație pozitivă între ExpectedYearsOfSchooling și LifeExpectancyAtBirth** (coeficient moderat, ex: 0.5-0.6): Educația este asociată cu o sănătate mai bună și o speranță de viață mai mare, ceea ce confirmă importanța investițiilor în educație pentru dezvoltarea umană.
# 
# - **Corelație negativă între Natalitate_(pop/nr,nasteri) și LifeExpectancyAtBirth** (coeficient negativ, ex: -0.4): Țările cu rate mari de natalitate tind să aibă o speranță de viață mai mică, ceea ce poate reflecta nivelul de dezvoltare socio-economică și accesul la servicii medicale.
# 
# - **Corelație slabă sau nesemnificativă între AccesElectricitate% și Populație**: Acest lucru sugerează că dimensiunea populației nu garantează automat un acces mai bun la infrastructură, ci contează și politicile publice și nivelul de dezvoltare.
# 
# ---
# 
# **Concluzie pentru analiza regresivă:**  
# Valorile ridicate ale corelațiilor între anumite variabile (ex: Populație și NrNasteri) impun atenție la multicoliniaritate. Corelațiile pozitive între indicatorii de infrastructură, educație și sănătate susțin ipoteza că acești factori sunt predictori importanți pentru dezvoltarea umană. Relațiile negative între natalitate și speranța de viață evidențiază impactul demografiei asupra calității vieții. Aceste observații trebuie integrate în selecția variabilelor pentru modelele de regresie, pentru a obține rezultate relevante și robuste.

# %%
plt.figure(figsize=(12, 6))
sns.boxplot(data=df.select_dtypes(include='number'))
plt.title("Boxplot pentru detectarea valorilor extreme")
plt.xticks(rotation=45)
plt.show()

# %% [markdown]
# ; Comentarii pentru fiecare boxplot al variabilelor cantitative:
# 
# ; - **AccesElectricitate%**: Boxplot-ul arată o distribuție puternic asimetrică spre valori mici, cu multe țări având acces redus la electricitate. Există outlieri spre valori mari, reprezentând țări cu infrastructură mai dezvoltată.
# 
# ; - **Populatie**: Distribuția populației este foarte dispersată, cu outlieri evidenți (țări cu populații foarte mari, precum China sau India). Majoritatea țărilor au populații mult mai mici, ceea ce duce la o cutie îngustă și mustăți lungi.
# 
# ; - **NrNasteri**: Similar cu populația, există câteva outlieri cu număr foarte mare de nașteri, iar majoritatea țărilor au valori mult mai mici. Cutia boxplot-ului este mică, iar mustățile sunt lungi.
# 
# ; - **Natalitate_(pop/nr,nasteri)**: Boxplot-ul arată variații moderate, cu câteva outlieri. Majoritatea țărilor se încadrează într-un interval relativ restrâns, dar există și țări cu rate de natalitate neobișnuit de mari sau mici.
# 
# ; - **LifeExpectancyAtBirth**: Distribuția speranței de viață este relativ concentrată, cu câteva outlieri spre valori mici (țări cu probleme majore de sănătate publică). Majoritatea țărilor au o speranță de viață apropiată de medie.
# 
# ; - **ExpectedYearsOfSchooling**: Boxplot-ul arată că majoritatea țărilor au un număr similar de ani așteptați de școlarizare, dar există și câteva outlieri cu valori foarte mici sau foarte mari, reflectând inegalități în accesul la educație.
# 
# ; Aceste observații evidențiază atât distribuția centrală, cât și extremele pentru fiecare variabilă, ajutând la identificarea potențialelor probleme sau particularități din date.

# %%
# Conversia coloanelor 'Populatie' și 'NrNasteri' la tip numeric (eliminând separatorii de mii)
df['Populatie'] = df['Populatie'].str.replace(',', '').astype(float)
df['NrNasteri'] = df['NrNasteri'].str.replace(',', '').astype(float)

# %%
# Populatie și NrNasteri sunt variabile cantitative, dar inițial au fost citite ca șiruri de caractere (string)
# deoarece conțin separatorul de mii (virgulă), de exemplu: "1,234,567".
# Pentru a putea face analize numerice (statistici, grafice, corelații), trebuie să le convertim la tip numeric (float).
# Conversia elimină separatorul de mii și transformă valorile în numere reale.

print(df[['Populatie', 'NrNasteri']].head())  


# %%
# Verificăm dacă există valori lipsă în baza de date
print("Număr de valori lipsă pentru fiecare coloană:")
print(df.isnull().sum())

# %%
# Comentariu asupra valorilor lipsă din baza de date

# Calculăm numărul de valori lipsă pentru fiecare coloană
valori_lipsa = df.isnull().sum()

print("Comentariu asupra valorilor lipsă:\n")
for col, nr_lipsa in valori_lipsa.items():
    if nr_lipsa > 0:
        print(f"- Coloana '{col}' are {nr_lipsa} valori lipsă dintr-un total de {len(df)} rânduri.")
if valori_lipsa.sum() == 0:
    print("Nu există valori lipsă în baza de date.")



# %%
#Prezența valorilor lipsă poate afecta analizele statistice și modelele predictive. 
# Este recomandat să investigăm cauzele acestora și să decidem dacă le completăm (imputare), le excludem sau le tratăm în alt mod, 
# în funcție de contextul analizei.

# %% [markdown]
# ## **1) Observăm că există valori lipsă**

# %% [markdown]
# Pentru tratarea problemei valorilor lipsă, grupăm țările în funcție de HDI. Țările cu un indice al dezvoltării umane asemănător au de obicei valori similare pentru restul variabilelor

# %%
# 1. Creare grupe după HDI

def clasificare_HDI(hdi):
    if hdi >= 0.8:
        return 'ridicat'
    elif hdi >= 0.65:
        return 'mediu'
    else:
        return 'scazut'

df['Nivel_dezvoltare'] = df['HumanDevelopmentIndex(HDI) '].apply(clasificare_HDI)

# %%
# 2. Imputarea medianei pe grupuri

for col in ['AccesElectricitate%', 'Populatie', 'NrNasteri', 'LifeExpectancyAtBirth', 'ExpectedYearsOfSchooling']:
    df[col] = df.groupby('Nivel_dezvoltare')[col].transform(lambda x: x.fillna(x.median()))

# %%
# 3. Verificăm dacă mai există valori lipsă
print("\n----- Valori lipsă pe coloană -----")
print(df.isnull().sum())

# %% [markdown]
# Din moment ce acum există o singură valoare lipsă în întreaga bază de date, o putem elimina, deoarece această observație nu are un impact puternic asupra analizei noastre.

# %%
df = df.dropna(subset=['HumanDevelopmentIndex(HDI) '])

# Verificăm dacă mai există valori lipsă
print("\n----- Valori lipsă pe coloană -----")
print(df.isnull().sum())

# %% [markdown]
# ### Metoda 2 de imputare a valorilor lipsă

# %%
# Imputarea valorilor lipsă: alegerea metodei

# Explicație:
# Deoarece numărul de valori lipsă pentru fiecare variabilă este relativ mic (sub 10% din totalul rândurilor),
# iar variabilele sunt în mare parte cantitative, cea mai potrivită metodă este imputarea cu media (mean) pentru variabilele numerice.
# Pentru variabilele de tip obiect (categorice sau string), putem folosi imputarea cu cea mai frecventă valoare (moda).
# Această abordare păstrează distribuția datelor și nu introduce bias semnificativ, fiind potrivită pentru analize statistice și modele predictive.

# Imputăm valorile lipsă pentru fiecare coloană
for col in df.columns:
    if df[col].dtype == 'float64' or df[col].dtype == 'int64':
        # Imputare cu media pentru variabile numerice
        df[col].fillna(df[col].mean(), inplace=True)
    elif df[col].dtype == 'object':
        # Imputare cu moda pentru variabile de tip obiect
        df[col].fillna(df[col].mode()[0], inplace=True)

print("Valorile lipsă au fost imputate folosind media (pentru variabile numerice) și moda (pentru variabile de tip obiect).")
print(df.isnull().sum())

# %% [markdown]
# ## Verificarea ipotezelor modelului de regresie

# %%

import numpy as np
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from scipy.stats import shapiro, normaltest
import warnings
warnings.filterwarnings("ignore")

# %% [markdown]
# ## Continuăm analiza descriptivă

# %%
# Matrice de corelație (numerice)
plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(numeric_only=True), annot=True, fmt=".2f", cmap="coolwarm")
plt.title("----- Corelația între variabile numerice -----")
plt.show()

# %%
# Histogramă pentru fiecare variabilă numerică

numeric_cols = df.select_dtypes(include=['float64']).columns
num_cols = len(numeric_cols)
cols = 4  # câte coloane de subploturi
rows = (num_cols + cols - 1) // cols  # câte rânduri sunt necesare

plt.figure(figsize=(15, 10))

for i, col in enumerate(numeric_cols, 1):
    ax = plt.subplot(rows, cols, i)
    ax.hist(df[col].dropna(), bins=20, edgecolor='black')
    ax.set_title(col)

    # Axa x: scoate notația științifică
    ax.ticklabel_format(style='plain', axis='x')
    ax.tick_params(axis='x', rotation=45)  # opțional: rotește etichetele dacă sunt lungi

plt.suptitle("----- Distribuții variabile numerice -----", fontsize=15)
plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()

# %% [markdown]
# ## 2) Observăm outlieri în cazul variabilelor *Populație* și *NrNasteri*

# %% [markdown]
# Am ales să rezolvăm problema valorilor aberante folosind logaritmarea

# %%
# Prelucrarea datelor din Populație

import numpy as np

df['Populatie_log'] = np.log1p(df['Populatie'])  # log(1 + x)

# Creăm graficele pentru a observa cum s-a schimbat distribuția valorilor
fig, axs = plt.subplots(1, 2, figsize=(14, 5))

# Histograma originală
axs[0].hist(df['Populatie'].dropna(), bins=30)
axs[0].set_title("Distribuția originală - Populatie")
axs[0].set_xlabel("Populatie")
axs[0].set_ylabel("Frecvență")
axs[0].ticklabel_format(style='plain', axis='x')  # scoate notatia 1e6

# Histograma transformată
axs[1].hist(df['Populatie_log'].dropna(), bins=30, color='orange')
axs[1].set_title("Distribuția log-transformată - Populatie_log")
axs[1].set_xlabel("log(1 + Populatie)")
axs[1].set_ylabel("Frecvență")

plt.suptitle("Compararea distribuției înainte și după transformarea logaritmică", fontsize=14)
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()


# %%
# Prelucrarea datelor din NrNasteri

df['NrNasteri_log'] = np.log1p(df['NrNasteri'])  # log(1 + x)

# Creăm graficele pentru a observa cum s-a schimbat distribuția valorilor
fig, axs = plt.subplots(1, 2, figsize=(14, 5))

# Histograma originală
axs[0].hist(df['NrNasteri'].dropna(), bins=30)
axs[0].set_title("Distribuția originală - NrNasteri")
axs[0].set_xlabel("NrNasteri")
axs[0].set_ylabel("Frecvență")
axs[0].ticklabel_format(style='plain', axis='x')  # scoate notatia 1e6

# Histograma transformată
axs[1].hist(df['NrNasteri_log'].dropna(), bins=30, color='orange')
axs[1].set_title("Distribuția log-transformată - NrNasteri_log")
axs[1].set_xlabel("log(1 + NrNasteri)")
axs[1].set_ylabel("Frecvență")

plt.suptitle("Compararea distribuției înainte și după transformarea logaritmică", fontsize=14)
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()


# %% [markdown]
# ## **Estimarea unui model de regresie liniară clasică**

# %% [markdown]
# Alegem ca variabile explicative rata populației cu acces la electricitate, populația (valoarea logaritmată), numărul de nașteri (valoarea logaritmată), speranța de viață, numărul așteptat de ani de școlarizare. Nu includem variabilele *natalitate* și *nivel de dezvoltare*, acestea fiind derivate din populație și numărul de nașteri, respectiv derivată din HDI

# %%
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import statsmodels.api as sm

# %%
# Selectarea variabilelor explicative
X = df[['AccesElectricitate%', 'Populatie_log', 'NrNasteri_log',
        'LifeExpectancyAtBirth', 'ExpectedYearsOfSchooling']]

# Variabila dependentă
y = df['HumanDevelopmentIndex(HDI) ']

# %%
# Analiza statistică a modelului de regresie liniară clasică OLS
X_const = sm.add_constant(X)
ols_model = sm.OLS(y, X_const).fit()
print("\nRezumat statsmodels:")
print(ols_model.summary())

# %% [markdown]
# În urma aplicării modelului OLS pe setul nostru de date, obținem un R-squared = 0.9 și Prob (F-statistic) = 1.36e-84, ceea ce arată o calitate crescută a modelului.
# 
# Cu excepția variabilei AccesElectricitate%, toate celelalte variabile explicative sunt semnificative statistic pentru un prag de 0.05, așadar variabila menționată poate fi exclusă din modelul OLS.
# 
# De asemenea, valoarea testului Durbin-Watson = 2.037 (foarte apropiată de 2) arată că reziduurile nu sunt autocorelate. Trebuie ținut cont și de faptul că reziduurile nu sunt distribuite normal (testul Jarque-Bera are o valoare foarte crescută)
# 
# Refacem modelul, de această dată fără variabila AccesElectricitate%

# %%
# Noua selecție de variabile explicative (fără AccesElectricitate%)
X_nou = df[['Populatie_log', 'NrNasteri_log', 'LifeExpectancyAtBirth', 'ExpectedYearsOfSchooling']]
y_nou = df['HumanDevelopmentIndex(HDI) ']  # ținta rămâne aceeași

# Analiza statistică a modelului de regresie liniară clasică OLS
X_const_nou = sm.add_constant(X_nou)
ols_model_nou = sm.OLS(y_nou, X_const_nou).fit()
print("\nRezumat statsmodels:")
print(ols_model_nou.summary())

# %% [markdown]
# Putem observa că eliminarea variabilei AccesElectricitate% a îmbunătățit interpretabilitatea modelului fără a-i reduce performanța. Toate cele 4 variabile explicative sunt acum semnificative statistic

# %% [markdown]
# ## **Estimarea unui model de regresie liniară prin învățare supervizată**

# %%
# Împărțirea în set de antrenare și test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Modelul de regresie
model = LinearRegression()
model.fit(X_train, y_train)

# Predicții
y_pred = model.predict(X_test)

# %%
# Evaluare
print("R² score:", r2_score(y_test, y_pred))
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("RMSE:", rmse)

# %% [markdown]
# Valoarea lui R-squared arată faptul că modelul explică 82,6% din variația indicelui de dezvoltare umană.

# %% [markdown]
# 
# Comparăm performanța testului de antrenare cu cea a setului de test:

# %%
# Scor pe setul de antrenare
y_train_pred = model.predict(X_train)
r2_train = r2_score(y_train, y_train_pred)
rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))

print("Train R²:", r2_train)
print("Train RMSE:", rmse_train)

# %% [markdown]
# Se observă o diferență de aproape 0,1 între R-squared pentru setul de antrenare și cel de test, iar valorile pentru RMSE nu diferă nici ele semnificativ, ceea ce indică faptul că nu este prezent cazul de overfitting.

# %%
# Coeficienți
coef_df = pd.DataFrame({
    'Variabilă': X.columns,
    'Coeficient': model.coef_
})
print("\nCoeficienți:")
print(coef_df)

# %%
# Statistici descriptive
print("\n----- Statistici descriptive pentru variabile numerice -----")
print(df.describe())

# %% [markdown]
# # Regresia polinomială

# %% [markdown]
# Vom folosi termeni polinomiali și interacțiuni între variabile pentru a captura relații non-lineare între predictori și variabila țintă.

# %%
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.model_selection import train_test_split

# %%
# Pregătirea datelor
X = df[['Populatie_log', 'NrNasteri_log', 'LifeExpectancyAtBirth', 'ExpectedYearsOfSchooling']]
y = df['HumanDevelopmentIndex(HDI) ']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# %%
# Construirea modelului polinomial grad 2
degree = 2
poly_model = make_pipeline(PolynomialFeatures(degree), LinearRegression())

# Antrenarea modelului
poly_model.fit(X_train, y_train)

# Predicții
y_pred = poly_model.predict(X_test)

# %%
# Evaluare
print(f"R² score: {r2_score(y_test, y_pred):.4f}")

mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
print(f"RMSE: {rmse:.4f}")

# %% [markdown]
# R-squared pentru regresia liniară simplă (0.9) este mai mare decât cel obținut folosind regresia polinomială (0.787)

# %% [markdown]
# # Random Forest

# %% [markdown]
# Random Forest este un model non-liniar, robust și des folosit pentru regresiile cu mai multe variabile explicative, pentru a captura relații complexe.

# %%
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.model_selection import train_test_split

# %%
# Datele selectate
X = df[['Populatie_log', 'NrNasteri_log', 'LifeExpectancyAtBirth', 'ExpectedYearsOfSchooling']]
y = df['HumanDevelopmentIndex(HDI) ']

# Împărțirea datelor în train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# %%
# Definirea modelului Random Forest
rf = RandomForestRegressor(n_estimators=100, random_state=42)

# Antrenarea modelului
rf.fit(X_train, y_train)

# Predicții pe test
y_pred = rf.predict(X_test)

# %%
# Evaluarea modelului
print(f"R² score: {r2_score(y_test, y_pred):.4f}")

mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
print(f"RMSE: {rmse:.4f}")

# %% [markdown]
# # Feature Selection

# %% [markdown]
# Pentru a îmbunătăți performanța modelelor și a obține o reprezentare mai relevantă a relației dintre variabilele explicative și indicele de dezvoltare umană (HDI), am explorat două abordări complementare:

# %% [markdown]
# 1. Eliminarea variabilelor informativ slabe
# 
# Am aplicat Regresia Lasso pentru a identifica cele mai importante variabile explicative. Acest model penalizează coeficienții irelevanți, împingându-i spre zero, ajutând astfel la selecția caracteristicilor relevante.

# %%
from sklearn.linear_model import LassoCV
from sklearn.preprocessing import StandardScaler

# Standardizare
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Lasso cu cross-validation
lasso = LassoCV(cv=5, random_state=42)
lasso.fit(X_scaled, y)

# Coeficienți
selected_features = pd.Series(lasso.coef_, index=X.columns)
print(selected_features)

# %% [markdown]
# 2. Crearea de variabile noi (Feature Engineering)
# Am creat o variabilă derivată, deja prezentă în set, dar ignorată anterior:
# 
# Natalitate relativă = Populație / Număr nașteri (notată Natalitate_(pop/nr.nasteri))
# 
# Aceasta exprimă o măsură inversă a ratei natalității, oferind o perspectivă interesantă asupra demografiei.
# 
# Am testat un model Random Forest folosind doar aceste trei variabile

# %%
X_fs = df[['Natalitate_(pop/nr.nasteri)', 'LifeExpectancyAtBirth', 'ExpectedYearsOfSchooling']]
y_fs = df['HumanDevelopmentIndex(HDI) ']

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X_fs, y_fs, test_size=0.2, random_state=42)

# Model
rf_fs = RandomForestRegressor(random_state=42)
rf_fs.fit(X_train, y_train)
y_pred = rf_fs.predict(X_test)

# Evaluare
print(f"R²: {r2_score(y_test, y_pred):.4f}")
print(f"RMSE: {mean_squared_error(y_test, y_pred)**0.5:.4f}")


# %% [markdown]
# ## Rezultatele obținute în urma aplicării modelelor alese
# 
# 
#   OLS_0:
# R-squared = 0.9
# Prob (F-statistic) = 385.2
# DW = 1.999
# JB = 671.969
# 
# 
#   Regresie liniara - supervizat antrenare:
# R-squared = 0.826
# RMSE = 0.0703
# 
#   Regresie liniara - supervizat test:
# R-squared = 0.921
# RMSE = 0.0411
# 
#   Regresia polinomială:
# R² score: 0.7874  RMSE: 0.0778
# 
#   Random forest:
# R² score: 0.7768
# RMSE: 0.0797


